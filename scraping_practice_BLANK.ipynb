{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first web scraping exercise\n",
    "\n",
    "We're going to scrape a bunch of separate lists from one Wikipedia page. Wikipedia is a good place to practice scraping because the HTML there is well formatted, and the site's traffic is so high, they don't mind us hitting one page again and again. \n",
    "\n",
    "First we import two libraries: [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is for scraping, and [Requests](http://docs.python-requests.org/en/master/user/quickstart/) is for making the HTTP request to the server where we want to scrape.\n",
    "\n",
    "**If you have not yet installed these two libraries** into the virtual environment where you are running this Notebook, you'll need to do that *first.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most scraping scripts begin with one page, which means one URL, to be scraped. In the three lines below, the only thing that changes if you scrape a different page, even at another website, is the URL inside the single quotaton marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_colleges_and_universities_in_Florida'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of those three lines above, we now have a BeautifulSoup \"object\" named `soup`, and it contains **all the HTML** from the file at the URL we gave. THe HTML is stored in a manner that allows the BeautifulSoup functions to search and use the data.\n",
    "\n",
    "Our goal is to scrape **all the lists** of colleges from that one Wikipedia page and put them in a CSV file, with separate columns for college name, location, type of college, and the URL for that college's Wikipedia page.\n",
    "\n",
    "We start by using Chrome Dev Tools' \"Inspect\": right-click on the heading \"State University System\" and select \"Inspect\" from the pop-up menu.\n",
    "\n",
    "If we inspect the heading above each of the lists of colleges and schools, we find that all of those headings are H3 headings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by collecting all the H3 elements on the page into a Python list\n",
    "h3_list = soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't really NEED to print them, but we do it so we can see what we got\n",
    "print(h3_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look closely, and you'll see that all the H3 heading elements are items in a Python *list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the heading above the first list of schools we want is 'State University System'\n",
    "# so let's get the first UL that comes after that heading\n",
    "the_ul = \"\"\n",
    "for head in h3_list:\n",
    "    if head.span.get_text() == 'State University System':\n",
    "        the_ul = head.find_next('ul')\n",
    "        break\n",
    "print(the_ul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, printing wasn't strictly necessary, but it's good to make sure you got what you were trying to get. Otherwise, if you start getting error messages, you won't understand why. You should be able to recognize that you got the complete UL element and everything it contains. It is NOT in a list. It is one single BeautifulSoup tag object, and it's been assigned to the variable `the_ul`.\n",
    "\n",
    "Above, you began with a *list* of all H3 elements on the page. Each H3 element from a Wikipedia page contains `span` tags and text and more. (You saw this with \"Inspect.\")\n",
    "\n",
    "The next task was to *get* the complete UL element that comes after the heading we chose: \"State University System.\" So we *looped* over the list of all H3 elements, and we checked *each one* to see if the text inside its *first* `span` tag matched the string we provided.\n",
    "\n",
    "As soon as we found that exact string, we stopped, and we grabbed *the next UL element* that comes after that H3, and we assigned that entire UL element to the variable `the_ul`.\n",
    "\n",
    "`break` makes the for-loop stop looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we want to get 3 things out of each LI element in that list: \n",
    "# college name, location, and the URL for that college's Wikipedia page \n",
    "# use Chrome Dev Tools' \"Inspect\" again to see the structure of the HTML and figure out how to use it\n",
    "\n",
    "schools = the_ul.find_all('li')\n",
    "\n",
    "for li in schools:\n",
    "    a_list = li.find_all('a')\n",
    "    college_name = a_list[0].get_text()\n",
    "    location = a_list[1].get_text()\n",
    "    url = a_list[0]['href']\n",
    "    print(college_name, location, url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 5 above - `schools = the_ul.find_all('li')` - makes a Python *list* of all the LI elements in the URL we selected. Once we have a list, we can *loop over it* and do things with each element, separately and individually. In this loop, `li` is the variable we made to represent each individual LI element in the list, which is named `schools`.\n",
    "\n",
    "One by one, for one LI at a time, we:\n",
    "\n",
    "* Find all the A elements in it and put them into a new list.\n",
    "* From the *first* A element, `a_list[0]`, we get the text inside the A tags and assign it to the variable `college_name`.\n",
    "* From the *second* A element, `a_list[1]`, we get the text inside the A tags and assign it to the variable `location`.\n",
    "* From the *first* A element, `a_list[0]`, we get the *value* of the `href` attribute and assign it to the variable `url`.\n",
    "* We print the three variables for each time the loop loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's only a partial URL, but it will work if we concatenate it with 'https://en.wikipedia.org'\n",
    "# let's do it over -\n",
    "for li in schools:\n",
    "    a_list = li.find_all('a')\n",
    "    college_name = a_list[0].get_text()\n",
    "    location = a_list[1].get_text()\n",
    "    url = 'https://en.wikipedia.org' + a_list[0]['href']\n",
    "    print(college_name, location, url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now how can we put them into a CSV file? Python has a built-in module named csv -\n",
    "# we have to import it to use it\n",
    "\n",
    "import csv\n",
    "\n",
    "# create/open new file for writing -\n",
    "csvfile = open(\"test.csv\", 'w', newline='', encoding='utf-8')\n",
    "\n",
    "# make a new variable, c, for Python's CSV writer object -\n",
    "c = csv.writer(csvfile)\n",
    "\n",
    "# write header row to test.csv\n",
    "c.writerow( ['first', 'second', 'third', 'fourth'] )\n",
    "\n",
    "# write two more rows to test.csv\n",
    "c.writerow( ['a', 'b', 'c', 'd'] )\n",
    "c.writerow( [10, 20, 30, 40] )\n",
    "\n",
    "\n",
    "# close and save the file\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now created a new CSV file and written three rows and four columns into it. But WHERE was it *saved*? To find out, use the `pwd` to find out which directory this Jupyter Notebook is running in. (`pwd` is a command that stands for \"print working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go there now in your Finder/File Explorer and open the CSV file named *test.csv.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to write a new CSV file. Can you figure out how to take the earlier code from above - the for-loop that PRINTS a list of schools - and make it WRITE to a row in the CSV *instead* of printing?\n",
    "\n",
    "Because that's all you need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/open new file for writing -\n",
    "csvfile = open(\"state_university_system.csv\", 'w', newline='', encoding='utf-8')\n",
    "\n",
    "# make a new variable, c, for Python's CSV writer object -\n",
    "c = csv.writer(csvfile)\n",
    "\n",
    "# write header row to the csv\n",
    "c.writerow( ['school', 'location', 'url'] )\n",
    "\n",
    "# the code from above that loops over all the schools in the list of LI elements\n",
    "for li in schools:\n",
    "    a_list = li.find_all('a')\n",
    "    college_name = a_list[0].get_text()\n",
    "    location = a_list[1].get_text()\n",
    "    url = 'https://en.wikipedia.org' + a_list[0]['href']\n",
    "    c.writerow( [college_name, location, url] )\n",
    "\n",
    "# close and save the file\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your new CSV, named *state_university_system.csv,* and take a look at the rows and columns.\n",
    "\n",
    "An **important point** to understand about the new line in the code above - `c.writerow( [college_name, location, url] )` - is that the square brackets *inside* the parentheses are **expected** by the csv module's code. It expects to receive a Python list (which is like an array in JavaScript), and if it doesn't, the CSV file will not be written correctly.\n",
    "\n",
    "So any time you WRITE a ROW to a CSV file here, you must write a *list* and NOT just variables separated by commas (which will not work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We said we would get all the schools from all the lists, and we will\n",
    "\n",
    "What we've done is nice, but it will be even nicer when we get all the lists of schools that are still in operation (we will skip the closed schools). And we already have almost all the Python and BeautifulSoup code we need to get the job done.\n",
    "\n",
    "If we are short on time for this demonstration, we will stop here.\n",
    "\n",
    "Recall that we got a list of all the H3 headings. It's in the variable `h3_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h3_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all the complete H3 elements. Let's look at just the text of the headings instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head in h3_list:\n",
    "    if head.span:\n",
    "        print(head.span.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check the Wikipedia page and discover I am missing \"Trade/technical institutions.\" I use \"Inspect\" again and see that the H3 element is a bit different from the others, so I try another way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head in h3_list:\n",
    "    if head.find( 'span', {'class':'mw-headline'} ):\n",
    "        print( head.find( 'span', {'class':'mw-headline'} ).get_text() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BeautifulSoup pattern `object.find( 'element', {'class':'classname'} )` is very, very common and very, very useful. \n",
    "\n",
    "`.find()` brings you only the first of the class, even if there are others on the page. \n",
    "\n",
    "`.find_all()` brings you a Python list of ALL the elements of that kind on the page that have that class.\n",
    "\n",
    "`.get_text()` strips the text only out of the element and gives it to you.\n",
    "\n",
    "When I examined the Wikipedia HTML carefully with \"Inspect,\" I realized that all the headings are inside a `span` tag that has `class=\"mw-headline` - *so that is what I used* to get the printed H3 headlines I want.\n",
    "\n",
    "Use Command-f (or Control-f on Windows) on the [BeautifulSoup documentation page](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to discover these tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy/paste that nice text to create a new list \n",
    "# we will use this list of headings to grab all the schools we want \n",
    "\n",
    "headings = [\n",
    "    'State University System',\n",
    "    'Florida College System',\n",
    "    'Other public institutions',\n",
    "    'Religiously affiliated institutions',\n",
    "    'Trade/technical institutions',\n",
    "    'Other private institutions'\n",
    "]\n",
    "\n",
    "# I removed 'Defunct public (county) colleges in Florida' and \n",
    "#'Segregated junior colleges' because those are all closed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've checked the list carefully against the Wikipedia page. Every **list of schools** that I want to add to my CSV comes under one of those headings. I will use the headings the same way I used 'State University System' alone, earlier, to get ONE list of schools.\n",
    "\n",
    "I will use the `h3_list` as I did before - but now I will ALSO use this new `headings` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in headings:\n",
    "    the_ul = \"\"\n",
    "    for head in h3_list:\n",
    "        text = head.find( 'span', {'class':'mw-headline'} ).get_text()\n",
    "        if text == item:\n",
    "            the_ul = head.find_next('ul')\n",
    "            break\n",
    "    # only the first LI in the UL \n",
    "    print( the_ul.li.get_text() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I printed only the text in the first LI in each list to test my code, and I have a problem. \"By state and in insular areas\" is not a school. Back on the Wikipedia page, I inspet again, and I find that the heading \"Other private institutions\" is followed (in the HTML) by a table, and *inside the table* there is a UL I do not want.\n",
    "\n",
    "This is a typical scraping problem, and we can solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in headings:\n",
    "    the_ul = \"\"\n",
    "    for head in h3_list:\n",
    "        text = head.find( 'span', {'class':'mw-headline'} ).get_text()\n",
    "        if text == item:\n",
    "            the_ul = head.find_next('ul')\n",
    "            # eliminate the ULs inside the table\n",
    "            if the_ul.parent.name == 'td':\n",
    "                the_ul = head.find_next('ul').find_next('ul').find_next('ul')\n",
    "            break\n",
    "    print( the_ul.li.get_text() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a clunky solution, but it works - as shown by the line \"Argosy University (Tampa),\" which is the first LI in that last list we want to grab. By the way, the trick with  `the_ul.parent.name` is something else I found on the [BeautifulSoup documentation page](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "`head.find_next('ul').find_next('ul').find_next('ul')` means not the first, not the second, get the third.\n",
    "\n",
    "Since we now know we can get all the LI elements, let's try to add in the code that gets the location and the URL too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in headings:\n",
    "    the_ul = \"\"\n",
    "    for head in h3_list:\n",
    "        text = head.find( 'span', {'class':'mw-headline'} ).get_text()\n",
    "        if text == item:\n",
    "            the_ul = head.find_next('ul')\n",
    "            # eliminate the ULs inside the table\n",
    "            if the_ul.parent.name == 'td':\n",
    "                the_ul = head.find_next('ul').find_next('ul').find_next('ul')\n",
    "            break\n",
    "    # print( the_ul.li.get_text() )\n",
    "    \n",
    "    schools = the_ul.find_all('li')\n",
    "\n",
    "    for li in schools:\n",
    "        a_list = li.find_all('a')\n",
    "        college_name = a_list[0].get_text()\n",
    "        location = a_list[1].get_text()\n",
    "        url = 'https://en.wikipedia.org' + a_list[0]['href']\n",
    "        print(college_name, location, url, item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, so let's insert that whole chunk of code *into* the CSV-writing code and give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/open new file for writing -\n",
    "csvfile = open(\"florida_colleges.csv\", 'w', newline='', encoding='utf-8')\n",
    "\n",
    "# make a new variable, c, for Python's CSV writer object -\n",
    "c = csv.writer(csvfile)\n",
    "\n",
    "# write header row to test.csv\n",
    "c.writerow( ['school', 'location', 'url', 'type'] )\n",
    "\n",
    "# write all the schools into rows\n",
    "for item in headings:\n",
    "    the_ul = \"\"\n",
    "    for head in h3_list:\n",
    "        text = head.find( 'span', {'class':'mw-headline'} ).get_text()\n",
    "        if text == item:\n",
    "            the_ul = head.find_next('ul')\n",
    "            # eliminate the ULs inside the table\n",
    "            if the_ul.parent.name == 'td':\n",
    "                the_ul = head.find_next('ul').find_next('ul').find_next('ul')\n",
    "            break\n",
    "    \n",
    "    schools = the_ul.find_all('li')\n",
    "\n",
    "    for li in schools:\n",
    "        a_list = li.find_all('a')\n",
    "        college_name = a_list[0].get_text()\n",
    "        location = a_list[1].get_text()\n",
    "        url = 'https://en.wikipedia.org' + a_list[0]['href']\n",
    "        \n",
    "        c.writerow( [college_name, location, url, item] )\n",
    "\n",
    "\n",
    "# close and save the file\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it works! You have a clean CSV with 120+ schools, and you could use it to make a map, a database, a mailing list (that would require more scraping on other pages), etc. You can sort the schools alphabetically or by location and still know which type of schools they are because of the \"type\" column. Some schools might not be real (it is Wikipedia, after all), so some more human intelligence would need to be applied to the CSV before it could be used with confidence.\n",
    "\n",
    "**Note** that *only the code in the final cell* (plus all the `import` statements from higher up) is needed to make this CSV. All the cells above that were testing and working to get to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
